# Теоретическая часть

## 1 Сверточные нейронные сети (CNN)

Сверточные нейронные сети (CNN, Convolutional Neural Networks) — это тип искусственных нейронных сетей, предназначенный для работы с данными, имеющими структуру сетки, например, изображениями. Они широко используются в задачах компьютерного зрения, таких как классификация изображений, сегментация и обнаружение объектов. Основная идея свертки заключается в применении фильтров (или ядер) для выявления признаков на изображениях, таких как края, текстуры или более сложные паттерны.

### Основные слои сверточной нейронной сети:

1. **Сверточный слой (Convolutional layer)**  
   Это основной слой сети, который применяет несколько свёрток с фильтрами к входным данным. Каждый фильтр извлекает различные признаки, такие как края, углы и текстуры. Выходом свёртки является набор активаций, представляющих обнаруженные паттерны.

2. **Пулинговый слой (Pooling layer)**  
   Пулинг используется для уменьшения размерности данных, сохраняя при этом важную информацию. Обычно используется операция максимального или усреднённого объединения, которая помогает снизить вычислительные затраты и количество параметров модели.

3. **Полносвязный слой (Fully connected layer)**  
   На этом этапе нейронная сеть принимает признаки, извлечённые на предыдущих слоях, и преобразует их в итоговое решение. Например, вероятность классов в задаче классификации.

4. **Функция активации (Activation function)**  
   После каждого сверточного и полносвязного слоя применяется функция активации (например, ReLU), которая вводит нелинейность в модель, позволяя сети решать более сложные задачи.

   Эта структура позволяет сверточным нейронным сетям эффективно обрабатывать и анализировать визуальные данные, обеспечивая высокую точность в задачах, связанных с изображениями.


## 2 Оптимизатор в CNN

Оптимизатор в сверточных нейронных сетях (CNN) используется для обновления весов модели во время обучения, с целью минимизации ошибки (функции потерь) и улучшения качества предсказания.

### Где используется оптимизатор в CNN:

1. **Сверточные слои**: Оптимизатор обновляет веса фильтров, которые извлекают признаки из изображений, такие как края и текстуры.
   
2. **Пулинговые слои**: Хотя пулинг не обучаемый, оптимизатор корректирует веса слоёв, перед и после пулинга, что влияет на обработку признаков.

3. **Полносвязные слои**: Эти слои комбинируют признаки для окончательной классификации. Оптимизатор корректирует их веса для улучшения точности.

4. **Обратное распространение ошибки**: Оптимизатор обновляет веса сети на основе разницы между предсказаниями и реальными метками, улучшая обучение.

### Как оптимизатор влияет на процесс обучения:

- **Скорость обучения (Learning Rate)**: Оптимизатор регулирует этот параметр, влияя на скорость изменения весов. Если скорость обучения слишком высокая, обучение может стать нестабильным, а если слишком низкая — процесс обучения будет медленным.

- **Сходимость**: Оптимизатор помогает сети достичь минимальной ошибки (функции потерь), ускоряя сходимость модели к оптимальному решению.

- **Использование градиентов**: Оптимизаторы, такие как **Adam**, используют информацию о градиентах функции потерь, чтобы корректировать веса модели более эффективно, что помогает избежать локальных минимумов и ускоряет обучение.

---

# Описание разработанной системы

## 1 Архитектура Inception

### Особенности:
#### Разреженная архитектура:
- Проблема: линейное наращивание сверточных слоев достаточно быстро себя исчерпывает.
- Идея: использование нелинейной разреженной архитектуры.

#### Конкатенация фильтров
- Проблема: при линейной структуре у нас признаки с рецептивного поля (область, которая участвует в вычислении данного нейрона) одного размера (ограниченного размером свертки).
- Идея: конкатенировать выходы сверток разного размера на слоях одной глубины.

#### Уменьшение сложности
- Проблема: при большом количестве карт много вычислений свертки.
- Идея: с помощью свертки  предварительно уменьшить количество карт.

<div align="center">
   <img src="https://github.com/user-attachments/assets/9f01c1ad-16f4-4596-bc3a-0b5b6c390947">
   <p>Рисунок 1 - Слои</p>
</div>


Сеть Inception состоит из множества Inception-блоков.

### Inception. Детали архитектуры
- Количество составных блоков около 100.
- Глубина 27 слоев, из них 22 - с обучаемыми параметрами.
- Перед последним полносвязным слоем - GAP.
- Введены два дополнительных классификатора в середине сети (борьба с затухающим градиентом, регуляризация).
- Функции потерь для дополнительных классификаторов домножаются на 0,3.
- Используется дропаут.

<div align="center">
   <img src="https://github.com/user-attachments/assets/8ffe8a22-8a42-4bd3-a08f-78e5d71c7843">
   <p>Рисунок 2 - Архитектура Inception</p> 
</div>


## 2 AdamW Optimizer

**AdamW** — это модификация популярного оптимизатора **Adam**. Основное отличие AdamW от обычного Adam заключается в том, как применяется регуляризация веса (weight decay), что делает обучение более стабильным и эффективным.

### Основные особенности:

1. **Адаптивный моментум**:
   - AdamW использует моменты первого и второго порядка для обновления весов, что помогает стабилизировать процесс обучения.
   
2. **Регуляризация веса (Weight Decay)**:
   - В AdamW регуляризация веса разделена от процесса обновления моментов, что позволяет лучше контролировать её влияние на веса.
   - В отличие от стандартного Adam, где регуляризация влияет на моменты, в AdamW регуляризация применяется непосредственно к весам.
   
3. **Быстрое и стабильное обучение**:
   - Разделение weight decay и обновления моментов улучшает стабильность и позволяет достигать лучших результатов в задачах машинного обучения, особенно в глубоких нейронных сетях.

### Как работает AdamW:

Стандартное обновление весов в Adam выглядит так:

```math
\theta_{t+1} = \theta_t - \eta \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}
```
где 𝑚<sub>𝑡<sub>  — это момент первого порядка (среднее значение градиента), а 𝑣<sub>𝑡<sub> — момент второго порядка (квадрат градиента).

- В AdamW регуляризация применяется отдельно, и шаг обновления выглядит так:
 
```math
\theta_{t+1} = \theta_t - \eta \cdot \frac{m_t}{\sqrt{v_t} + \epsilon} - \eta \cdot \lambda \cdot \theta_t
```
где 𝜆 — это коэффициент регуляризации (weight decay), который применяется только к весам.

Преимущества AdamW:
- Более стабильное обучение: из-за разделения weight decay и оптимизации параметров.
- Эффективная регуляризация: поскольку регуляризация теперь полностью контролируется и не влияет на моменты.
- Популярность в современных моделях: AdamW стал предпочтительным выбором для многих современных моделей, таких как трансформеры, благодаря своей эффективности в обучении.

Таким образом, AdamW улучшает стандартный Adam, предоставляя более точное и эффективное управление регуляризацией и стабилизируя обучение, особенно в случаях, когда важна высокая производительность модели.

---

# Результаты работы и тестирования системы
На рисунке 3 представлен результат обучения нейронной сети Inception с оптимизаторами Adam и AdamW. 

<div align="center">
   <img src="https://github.com/user-attachments/assets/146e44b6-c76f-44b4-8c68-06e5043461e5">
   <img src="https://github.com/user-attachments/assets/f4dcb344-1243-438b-b986-9abf6df8a051">
   <p>Рисунок 3 - Результат обучения</p> 
</div>

Графики потерь и точности приведены на рисунках 4-6.

<div align="center">
   <img src="https://github.com/user-attachments/assets/9715d05b-6395-4a1d-a40e-40fc3a6b2845">
   <p>Рисунок 4 - График точности</p>
</div>


<div align="center">
   <img src="https://github.com/user-attachments/assets/4a1d9cd8-3bdf-4059-8184-730beca45a33">
   <p>Рисунок 5 - График точности</p>
</div>


<div align="center">
   <img src="https://github.com/user-attachments/assets/1379bfae-d0ee-42cd-99ec-22f13c592ba3">
   <p>Рисунок 6 - График потерь</p>
</div>

---

# Выводы по работе
В результате выполнения лабораторной работы была реализована нейронная сеть Inception с оптимизатором AdamW. 
- Точность модели с оптимизаторам Adam составила 52%.
- Точность модели с оптимизатором AdaSmooth составила 57%.

---

# Использованные источники
- https://aisec.cs.msu.ru/section_robust_ml/nn_architectures/
- https://habr.com/ru/articles/301084/
